{"register_class": "required a registered Evaluator class key here, default as 'base_evaluator'", "exp_name": "you can speficify the name of the exp you want, or keep it empty to use the timestamp + config_file_name", "model_configs": [{"register_class": "required a registered LCWAugModel (sub)class key here", "model_name": "required a specific model's name you like here", "model_path": "required a model loading path here", "tokenizer_path": "the tokenizer loading path here, you can keep it empty to use the corresponding model's path", "use_cache": "'true' / 'false' to specify whether to use the cache in model.generate call or not, default 'true'", "use_fast_tokenizer": "whether to use the fast rust-based tokenizer, default 'true', setting if to 'false' will use python-based tokenizer", "max_prompt_length": "the maximum length of the prompt words, if over, truncate it, default as 32k, and -1 means no limit", "torch_dtype": "the torch dtype string of the model you want, default using float16, you can also use bfloat16, etc", "device_map": "the device map distribution, default using 'auto', or you can pass 'single' to use the only first gpu, or more generally, pass the dict-like device map to specify", "tensor_parallel": "whether to use tensor parallel, default 'false' for not using", "decode_strategy": "the string to set how to decode the output text from the model's logits, default using 'greedy', the other choices are 'beam', 'topk_sampling', 'topp_sampling', etc, depending on the model", "aug_method": "you can set the aug method here, or keep it empty for no aug", "[additional config]": "any basic python type"}], "dataset_configs": [{"register_class": "required a registered LCWDataset (sub)class key here", "dataset_name": ["required the specific dataset's loading name(s) here, following the hugging-face style"], "dataset_path": "required the specific dataset's loading path here, default loading from the huggingface.co if empty", "split": "the split of the dataset to be actually used, default as 'test' ", "batch_size": "the batched sample number to be used during one iteration, default as 1(int)", "prompt_template": "you can set the specific prompt template you need here, or keep it empty to let it be generated automatically probably", "max_new_tokens": "the maximum number of new tokens to be generated by the model for this dataset, -1 means no limit, default using 0 to let dataset handle it", "length_splits": ["the lengths of the different length split ranges, default as [1000, 2000, 4000, 8000, 16000](List[int])"], "metrics": ["the metrics used for evaluating the model, you can keep it empty list to use the metrics defined in the dataset itself"], "main_metric": "the main metric for evaluating the model, keep it empty to default use the first metric in the score output", "ppl_metric": "the metric used for evaluating the model's perplexity, default 'false' to not evaluate it, since it may cost twice forward", "few_shot": "the number of few-shot examples, default set 0, i.e. zero-shot setting", "few_shot_split": "the split of the dataset for few-shot setting, default as 'dev' ", "few_shot_prefix": "the few-shot example prefix prompt, default empty, but should be set when using few-shot setting, maybe specific to dataset", "few_shot_suffix": "the few-shot example suffix prompt, default empty, but should be set when using few-shot setting, maybe specific to dataset", "[additional config]": "any basic python type"}]}